<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>ChatGPT原理科普 | JrunDing</title><meta name="author" content="JrunDing"><meta name="copyright" content="JrunDing"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="一	引言1.1	ChatGPT简介​	ChatGPT，全称 Chatbot based on Generative Pre-trained Transformers，是一种基于 GPT 架构的开放领域生成式对话机器人。通过构建在大量文本数据上预训练的语言模型，ChatGPT 能够理解自然语言并生成连贯、有意义的回复。与传统的基于规则或检索的对话系统相比，ChatGPT 更具灵活性和创造性，能够在多">
<meta property="og:type" content="article">
<meta property="og:title" content="ChatGPT原理科普">
<meta property="og:url" content="http://jrunding.github.io/2023/09/12/ChatGPT%E5%8E%9F%E7%90%86%E7%A7%91%E6%99%AE/index.html">
<meta property="og:site_name" content="JrunDing">
<meta property="og:description" content="一	引言1.1	ChatGPT简介​	ChatGPT，全称 Chatbot based on Generative Pre-trained Transformers，是一种基于 GPT 架构的开放领域生成式对话机器人。通过构建在大量文本数据上预训练的语言模型，ChatGPT 能够理解自然语言并生成连贯、有意义的回复。与传统的基于规则或检索的对话系统相比，ChatGPT 更具灵活性和创造性，能够在多">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s1.ax1x.com/2023/03/03/ppkgPxK.jpg">
<meta property="article:published_time" content="2023-09-12T12:51:19.000Z">
<meta property="article:modified_time" content="2024-09-30T08:18:04.707Z">
<meta property="article:author" content="JrunDing">
<meta property="article:tag" content="Communication, Automation, ML, DL, NLP, CV, Movie, photography">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s1.ax1x.com/2023/03/03/ppkgPxK.jpg"><link rel="shortcut icon" href="https://z1.ax1x.com/2023/11/13/piJMDnH.jpg"><link rel="canonical" href="http://jrunding.github.io/2023/09/12/ChatGPT%E5%8E%9F%E7%90%86%E7%A7%91%E6%99%AE/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: JrunDing","link":"链接: ","source":"来源: JrunDing","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ChatGPT原理科普',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-30 16:18:04'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://z1.ax1x.com/2023/11/13/piJMDnH.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">243</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/biography/"><span> Biography</span></a></div><div class="menus_item"><a class="site-page" href="/category/"><span> Category</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/movie/"><span> Movie</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="JrunDing"></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/biography/"><span> Biography</span></a></div><div class="menus_item"><a class="site-page" href="/category/"><span> Category</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/movie/"><span> Movie</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">ChatGPT原理科普</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-09-12T12:51:19.000Z" title="发表于 2023-09-12 20:51:19">2023-09-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-30T08:18:04.707Z" title="更新于 2024-09-30 16:18:04">2024-09-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Natural-Language-Processing/">Natural Language Processing</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>15分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="ChatGPT原理科普"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><span class="disqus-comment-count"><a href="http://jrunding.github.io/2023/09/12/ChatGPT%E5%8E%9F%E7%90%86%E7%A7%91%E6%99%AE/#disqus_thread"><i class="fa-solid fa-spinner fa-spin"></i></a></span></span></div></div></div><article class="post-content" id="article-container"><h1 id="一引言"><a href="#一引言" class="headerlink" title="一	引言"></a>一	引言</h1><h2 id="1-1ChatGPT简介"><a href="#1-1ChatGPT简介" class="headerlink" title="1.1	ChatGPT简介"></a>1.1	ChatGPT简介</h2><p>​	ChatGPT，全称 Chatbot based on Generative Pre-trained Transformers，是一种基于 GPT 架构的开放领域生成式对话机器人。通过构建在大量文本数据上预训练的语言模型，ChatGPT 能够理解自然语言并生成连贯、有意义的回复。与传统的基于规则或检索的对话系统相比，ChatGPT 更具灵活性和创造性，能够在多种应用场景中提供更为自然的人机交互体验。</p>
<p>​	ChatGPT的发展受益于深度学习、自然语言处理领域的最新技术，如Transformer结构、预训练与微调的策略等。借助这些技术，ChatGPT在语言理解、文本生成等多个任务上取得了显著的性能提升。同时，ChatGPT已经在众多实际应用中展现出强大的潜力，如客户服务、语言教育、虚拟助手等。</p>
<p>​	然而，ChatGPT 仍面临一些挑战，如安全性与道德风险、模型运行效率、个性化与多样性等。未来的发展将致力于解决这些问题，为用户带来更加智能、高效的人工智能对话体验。</p>
<h2 id="1-2语言模型发展背景"><a href="#1-2语言模型发展背景" class="headerlink" title="1.2	语言模型发展背景"></a>1.2	语言模型发展背景</h2><p>​	语言模型（Language Model）是自然语言处理领域的核心技术之一，旨在学习自然语言的结构和概率分布。随着深度学习技术的发展，语言模型取得了显著的进步，从最初的基于 n-gram 的统计模型，到基于循环神经网络（RNN）和长短时记忆网络（LSTM）的神经网络模型，再到近年来的基于 Transformer 架构的预训练模型，如 BERT、GPT 等。</p>
<p>​	这里面 RNN（Recurrent Neural Network）：循环神经网络 RNN 是一种特殊的神经网络结构，其能够处理不定长的序列输入。RNN 的关键创新在于引入了循环连接，使得网络在处理序列数据时可以保留之前的历史信息。然而，RNN 存在梯度消失和梯度爆炸问题，这使得网络难以学习长距离依赖关系。</p>
<p>​	所以后面出现一个 LSTM（Long Short-Term Memory）：长短时记忆网络 LSTM 是 RNN 的一种变体，专门设计用来解决梯度消失和梯度爆炸问题。LSTM 通过引入门控单元（gating units）和记忆细胞（memory cells），使得网络能够有选择地保留或遗忘历史信息。这使得 LSTM 在处理长距离依赖关系方面表现优越。</p>
<p>​	后续在 NLP 的发展过程中，一些关键技术的出现推动了语言模型性能的突破。例如，神经网络模型成功解决了 n-gram 模型中数据稀疏性和长距离依赖问题；而 Transformer 架构通过自注意力机制（Self-Attention Mechanism）提高了计算并行性，大幅提升了模型的训练效率。	</p>
<p>​	近年来，<strong>预训练模型成为了自然语言处理领域的核心技术</strong>。通过在大规模文本数据上进行无监督预训练，模型可以学习到丰富的语言知识。再结合任务相关的有监督数据进行微调，预训练模型可以在各种自然语言处理任务上取得优异表现。GPT 系列模型作为预训练模型的代表之一，尤其在生成任务方面表现突出。</p>
<h1 id="二ChatGPT核心技术"><a href="#二ChatGPT核心技术" class="headerlink" title="二	ChatGPT核心技术"></a>二	ChatGPT核心技术</h1><p>​	预训练Transformer模型主要以BERT、GPT模型为主。GPT发展有GPT-1、GPT-2、GPT-3，以及GPT-3发展而来的InstructGPT和ChatGPT。</p>
<h2 id="2-1GPT-Generative-Pre-trained-Transformer-模型介绍"><a href="#2-1GPT-Generative-Pre-trained-Transformer-模型介绍" class="headerlink" title="2.1	GPT(Generative Pre-trained Transformer)模型介绍"></a>2.1	GPT(Generative Pre-trained Transformer)模型介绍</h2><p>​	GPT 是一种生成式预训练 Transformer 模型，由 OpenAI 团队在2018年首次提出。</p>
<p>​	GPT 模型是一种自然语言处理（NLP）模型，使用多层变换器（Transformer）来预测下一个单词的概率分布，通过训练在大型文本语料库上学习到的语言模式来生成自然语言文本。</p>
<p>​	GPT 是预训练模型，是基于一些基础模型演变而来的，如 GPT-2 是基于 GPT-1 之上训练而来，GPT-3 是基于 GPT-2 而来。随着版本的升级，模型的层数、参数数量和预训练数据规模都在不断增加，从而使GPT在各种任务上取得更好的性能。尤其是GPT-3，其具有超过1750亿个参数，被认为是自然语言处理领域的一项重要突破。</p>
<p>​	GPT-3通俗解释: GPT-3模型是计算机在拥有3000亿单词的⽂本语料基础上训练出的拥有1750亿参数的模型。这⾥⾯3000亿单词就是训练数据， ⽽1750亿参数就是沉淀下来的 AI 对这个世界的理解，这个理解包括了对⼈的语⾔⽂法的理解，也包括 ⼈类世界的具体知识。ChatGPT 使⽤新训练⽅法调整了 GPT-3模型，并将 GPT-3模型的参数量压缩了 100多倍，还拥有了对话聊天的能⼒。</p>
<p>​	和以往的AI对话机器人不同之处：⼀个通⽤超⼤模型尝试解决⼈类世界的各种问题，⽽不是由写 诗模型、写代码模型、写⼴告模型组合起来的⼀个家伙。所以“他”仿佛具备了像⼈⼀样的通⽤能 ⼒，并且这个通⽤能⼒在某些⽅⾯还⾜够的好。</p>
<p>​	GPT-3.5在2022年11⽉底进⾏了两个更新，分别是 text-davinci-003和 ChatGPT，其中 ChatGPT 以对话形式进⾏交互，既能回答问题也能承认错误、质疑不正确的前提以及拒绝不恰当的请求。</p>
<h2 id="2-2预训练与微调-Pre-training-and-Fine-tuning"><a href="#2-2预训练与微调-Pre-training-and-Fine-tuning" class="headerlink" title="2.2	预训练与微调(Pre-training and Fine-tuning)"></a>2.2	预训练与微调(Pre-training and Fine-tuning)</h2><p>​	Transformer 的训练分为两个阶段：预训练和微调。在预训练阶段，模型在大量的文本数据上进行无监督学习，以捕捉语言知识。在微调阶段，模型在特定的对话数据集上进行有监督训练，以优化对话生成性能。</p>
<p>​	BERT 和 GPT 主要的区别是预训练任务方式不同：BERT 的训练目标是通过上下文预测缺失的单词（掩码语言模型），而 GPT 的训练目标是预测给定上文的下一个单词（自回归语言模型）。BERT使用双向Transformer编码器，同时考虑上下文来学习单词的表示。这使得BERT能够捕捉到更丰富的语义信息。相反，GPT使用单向Transformer解码器，仅根据左侧上文来生成下一个单词，这使得GPT在某些需要充分利用上下文信息的任务上可能表现较差。</p>
<p>​	虽然这篇主要是讲 ChatGPT 背后的原理，但是为了让大家更好地理解：这里会写出 BERT 模型和 GPT 模型的训练区别。</p>
<h3 id="2-3-1预训练任务-Pre-training-Tasks"><a href="#2-3-1预训练任务-Pre-training-Tasks" class="headerlink" title="2.3.1	预训练任务(Pre-training Tasks)"></a>2.3.1	预训练任务(Pre-training Tasks)</h3><p>​	在自然语言处理领域，预训练任务是指在进行特定任务（如文本分类、问答系统等）之前，对模型进行大量无标签数据的训练，使其学会基本的语言知识。预训练任务的目的是为了让模型在之后的下游任务中有更好的性能表现。</p>
<p>​	<strong>BERT 模型训练任务</strong></p>
<p>​	BERT 主要训练⽅法是做完型填空，在训练过程中对于某个词的学习要观测当前词的所有上下⽂更新模型参数，可以认为模型在观测了⼤量⽂本语料之后建 ⽴了每个词和其他的词之间的关系。</p>
<p>​	BERT常见的预训练任务包括：掩码语言模型（Masked Language Model, MLM）、下一句预测（Next Sentence Prediction, NSP）等。接下来我们详细介绍这些预训练任务。</p>
<p>​	1）掩码语言模型（Masked Language Model, MLM）</p>
<p>​	掩码语言模型是一种以自监督的方式学习句子中词汇和语法结构的任务。在训练过程中，输入句子的一部分单词会被随机替换成掩码符号（如[MASK]）。模型的目标是基于上下文信息预测被掩码的单词。通过这种方式，模型能够学习到句子中的词汇和语序关系。这有助于模型更好地理解句子的结构和语义信息。</p>
<p>​	例如，给定句子 “The cat is playing with a toy”，MLM 任务可能将其变为 “The [MASK] is playing with a toy”，模型需要预测被掩码的单词是 “cat”。</p>
<p>​	2）下一句预测（Next Sentence Prediction, NSP）</p>
<p>​	下一句预测任务主要用于模型学习句子间的关系。在训练过程中，模型接收两个句子作为输入，需要预测第二个句子是否紧跟在第一个句子之后。这样，模型可以学习到句子间的逻辑关系和上下文信息。这有助于模型理解段落和长文本的结构。</p>
<p>​	以这两个句子为例：”The weather is nice today. I will go for a walk.”，模型需要预测这两个句子是否相邻。在反例中，模型可能接收到的句子为：”The weather is nice today. I had pizza for dinner.”，此时模型需要预测这两个句子不是相邻的。</p>
<p>​	<strong>GPT 模型训练任务</strong></p>
<p>​	⽽GPT的训练⽅向更像是写⼩作⽂，对某⼀个词的预测只依赖当 前词之前的所有⽂本，⽽看不到词之后的内容，所以GPT⼜叫⽂本⽣成模型。</p>
<p>​	在预训练阶段，GPT 使用大规模的无标签文本数据集进行训练，学习自然语言的一般规律，例如语法、句法、语义和共现信息等。GPT 采用单向自回归语言模型（Autoregressive Language Model），通过最大化给定上文的条件概率来预测下一个词。下面是一个简化的例子：</p>
<p>​	假设我们有一个文本数据集，其中有一句话：“今天天气很好，我们去公园玩吧。”我们的目标是训练GPT模型能够预测给定上文的下一个词。</p>
<p>​	1）首先，将文本进行分词处理，将句子拆分成单词序列：[“今天”, “天气”, “很好”, “，”, “我们”, “去”, “公园”, “玩”, “吧”, “。”]</p>
<p>​	2）接下来，根据单词序列生成训练样本，每个样本包含输入序列（上文）和目标序列（下一个词）。例如：</p>
<ul>
<li>输入序列：“今天”，目标序列：“天气”</li>
<li>输入序列：“今天 天气”，目标序列：“很好”</li>
<li>输入序列：“今天 天气 很好”，目标序列：“，”</li>
</ul>
<p>​	3）使用 Masked Self-Attention 和 Transformer 架构，训练模型根据输入序列预测目标序列。例如，给定输入序列“今天天气很好”，模型的输出应尽可能接近目标序列“，”。</p>
<p>​	4）在整个预训练过程中，模型会不断地调整参数，以最大化输入序列与目标序列之间的条件概率。这样，当预训练完成后，GPT 模型就能够捕捉到自然语言中的语法、句法、语义等规律。</p>
<p>​	需要注意的是，这个例子仅是为了说明GPT预训练阶段的基本过程，实际训练中会在大规模的文本数据集上进行，并且采用更复杂的分词和训练技巧。</p>
<h3 id="2-3-2微调任务-Fine-tuning-Tasks"><a href="#2-3-2微调任务-Fine-tuning-Tasks" class="headerlink" title="2.3.2	微调任务(Fine-tuning Tasks)"></a>2.3.2	微调任务(Fine-tuning Tasks)</h3><p>​	在预训练阶段完成后，ChatGPT 需要在特定任务上进行微调，以提高在该任务中的性能。微调是一个有监督学习过程，需要针对特定任务的训练数据集。一些常见的微调任务有：问答任务（Question-Answering, QA）、情感分析（Sentiment Analysis）、文本摘要（Text Summarization）、机器翻译（Machine Translation）、任务导向对话（Task-Oriented Dialogue）。以下以问答任务作为例子：</p>
<p>​	问答任务（Question-Answering, QA） 在问答任务中，ChatGPT 从给定的上下文中提取答案，以回应用户提出的问题。微调数据集包含问题和对应答案的配对，例如 SQuAD、CoQA 和 QuAC 等。通过在这些数据集上进行微调，ChatGPT 能够提高在问答任务中的准确性和效果。</p>
<p>​	当然可以。以下是一个问答任务的例子：</p>
<p>​	假设我们有一段文字作为上下文：</p>
<p>​	上下文：</p>
<p>​	《哈利·波特与魔法石》是英国作家J.K.罗琳创作的奇幻小说系列的第一部。这部小说的主人公是一个年轻的巫师哈利·波特，他在霍格沃茨魔法学院学习魔法。哈利与他的朋友赫敏·格兰杰和罗恩·韦斯莱一起揭示了魔法石的秘密，阻止了邪恶的伏地魔企图获得永生。</p>
<p>​	问题：</p>
<p>​	哈利·波特与魔法石的作者是谁？</p>
<p>​	预期回答：</p>
<p>​	J.K.罗琳</p>
<p>​	在这个问答任务中，输入的问题是“哈利·波特与魔法石的作者是谁？”，ChatGPT 需要从提供的上下文中找到正确的答案。在这个例子中，上下文明确提到了J.K.罗琳是这部小说的作者，因此预期回答是“J.K.罗琳”。通过在问答任务上进行微调，ChatGPT 可以提高在类似场景中准确回答问题的能力。</p>
<p>​	<strong>总结：BERT和GPT虽然都是基于Transformer的预训练语言模型，但它们在训练目标、文本处理方式以及应用场景上有所不同。BERT注重双向上下文建模，适用于自然语言理解任务；GPT则以生成能力为主，更适用于自然语言生成任务。</strong></p>
<h2 id="2-3什么是参数"><a href="#2-3什么是参数" class="headerlink" title="2.3	什么是参数"></a>2.3	什么是参数</h2><p>​	为什么 ChatGPT 或者其他大语言模型以参数为量级，衡量大语言是否优越性等？</p>
<p>​	在大语言模型中，参数主要是神经网络中的权重和偏置。神经网络由多个层组成，每层包含许多神经元。这些层之间的连接由权重和偏置决定。权重和偏置是模型根据训练数据自动学习并调整的数值。参数就是神经元之间的连接，即参数就是这些权重和偏置。</p>
<p>​	<strong>所以参数越多，即意味该大语言模型能处理的任务越多。</strong> 大型语言模型，如 OpenAI 的 GPT-3，通常具有数千亿个参数。这些参数使得模型能够学习到更丰富的知识和更复杂的模式，从而提高其在各种任务中的表现。然而，大量参数也意味着更高的计算资源和存储需求，以及潜在的过拟合风险。</p>
<h2 id="2-4最后基于GPT出来的ChatGPT"><a href="#2-4最后基于GPT出来的ChatGPT" class="headerlink" title="2.4	最后基于GPT出来的ChatGPT"></a>2.4	最后基于GPT出来的ChatGPT</h2><p>​	ChatGPT 是 Openai 基于 InstustGPT 微调而来。ChatGPT 是具备理解上下文、连贯性等诸多先进特征，有着多种应用场景的**聊天机器人。2022年11⽉30⽇由 OpenAI 发布。</p>
<p>​	ChatGPT 目前能力范围可以覆盖<strong>回答问题、撰写文章、文本摘要、语言翻译和生成计算机代码</strong>等任务。</p>
<p>​	现在看看业界中文的大语言模型排行榜：</p>
<p>​	<strong>排行榜会定期更新，可访问：</strong><br>​	<a target="_blank" rel="noopener" href="https://www.cluebenchmarks.com/">https://www.cluebenchmarks.com</a><br>​	<a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/SuperCLUE">https://github.com/CLUEbenchmark/SuperCLUE</a></p>
<h1 id="三ChatGPT的优势与挑战"><a href="#三ChatGPT的优势与挑战" class="headerlink" title="三	ChatGPT的优势与挑战"></a>三	ChatGPT的优势与挑战</h1><h2 id="3-1优势"><a href="#3-1优势" class="headerlink" title="3.1	优势"></a>3.1	优势</h2><h3 id="3-1-1强大的语言理解与生成能力"><a href="#3-1-1强大的语言理解与生成能力" class="headerlink" title="3.1.1	强大的语言理解与生成能力"></a>3.1.1	强大的语言理解与生成能力</h3><p>​	由于 GPT 模型在预训练阶段学习了大量的语言知识，因此 ChatGPT 具有强大的语言理解与生成能力。这使得 ChatGPT 可以更好地理解用户的意图，生成更加准确和丰富的回复。</p>
<p>​	除了基本的语法和句法知识外，ChatGPT 还具有深度的语义理解能力。这意味着它可以在更高层次上理解用户的问题和需求，为用户提供更加精准的回答和建议。</p>
<p>​	得益于 GPT 模型的强大生成能力，ChatGPT 可以根据用户的问题和场景生成多样化的回答。这不仅使得它能够满足用户的个性化需求，还能为用户提供更加丰富多彩的聊天体验。</p>
<p>​	ChatGPT 不仅适用于聊天场景，还可以应用于其他任务，如文本摘要、问答系统、文本生成等。这意味着它可以为用户提供更加全面的服务，满足用户在不同场景下的需求。</p>
<p>​	<strong>使用 ChatGPT+Prompt 工程，可以快速替换传统的技术学习模型。特别是分类问题。</strong></p>
<h3 id="3-1-2适应多领域应用"><a href="#3-1-2适应多领域应用" class="headerlink" title="3.1.2	适应多领域应用"></a>3.1.2	适应多领域应用</h3><p>​	和以往的AI对话机器人不同之处：⼀个通⽤超⼤模型尝试解决⼈类世界的各种问题，⽽不是由写 诗模型、写代码模型、写⼴告模型组合起来的⼀个家伙。所以“他”仿佛具备了像⼈⼀样的通⽤能 ⼒，并且这个通⽤能⼒在某些⽅⾯还⾜够的好。</p>
<h2 id="3-2挑战"><a href="#3-2挑战" class="headerlink" title="3.2	挑战"></a>3.2	挑战</h2><h3 id="3-2-1安全性与道德风险"><a href="#3-2-1安全性与道德风险" class="headerlink" title="3.2.1	安全性与道德风险"></a>3.2.1	安全性与道德风险</h3><p>​	1）ChatGPT 产生的答复是否产生相应的知识产权？ChatGPT 进行数据挖掘和训练的过程是否需要获得相应的知识产权授权？<br>​	2）ChatGPT 是基于统计的语言模型，这一机制导致回答偏差会进而导致虚假信息传播的法律风险，如何降低其虚假信息传播风险？</p>
<h3 id="3-2-2时效性和一本正经胡说八道"><a href="#3-2-2时效性和一本正经胡说八道" class="headerlink" title="3.2.2	时效性和一本正经胡说八道"></a>3.2.2	时效性和一本正经胡说八道</h3><p>​	1）ChatGPT 的回答可能过时，因为其数据库内容只到2021年，对于涉及2022年之后，或者在2022年有变动的问题无能为力</p>
<p>​	2）ChatGPT 在专业较强的领域无法保证正确率，即使在鸡兔同笼此类初级问题中仍然存在错误，并且英文回答和中文回答存在明显差异化</p>
<p>​	3）ChatGPT 对于不熟悉的问题会强行给出一定的答案，即使答案明显错误，依然会坚持下去，直到明确戳破其掩饰的内容，会立马道歉，但本质上会在不熟悉的领域造成误导</p>
<h3 id="3-2-3模型运行效率"><a href="#3-2-3模型运行效率" class="headerlink" title="3.2.3	模型运行效率"></a>3.2.3	模型运行效率</h3><p>​	1）生成的字越多，回复效率越慢。<br>​	2）并发量低，费用高。</p>
<h1 id="四总结"><a href="#四总结" class="headerlink" title="四	总结"></a>四	总结</h1><p>​	语言模型发展背景：从基于 n-gram 的统计模型到基于 RNN 和 LSTM 的神经网络模型，再到基于 Transformer 架构的预训练模型，如 BERT 和 GPT。</p>
<p>​	ChatGPT 的核心技术包括：GPT 模型、预训练与微调策略等。GPT 模型利用多层变换器进行自然语言文本生成；预训练阶段在大量文本数据上学习语言知识，微调阶段在特定任务数据集上进行优化。</p>
<p>​	BERT 和 GPT，它们在训练目标、文本处理方式和应用场景上有所区别。BERT 注重双向上下文建模，适用于自然语言理解任务；GPT 以生成能力为主，适用于自然语言生成任务。</p>
<p>​	参数主要是神经网络中的权重和偏置，数量越多意味着模型能处理的任务越多，学习到更丰富的知识和更复杂的模式。但大量参数也意味着更高的计算资源和存储需求以及潜在的过拟合风险。</p>
<p>​	ChatGPT 作为一种基于 GPT 模型的聊天机器人，具有强大的语言理解和生成能力，能够适应多种任务和场景，为用户提供个性化和多样化的回答。随着人工智能技术的不断发展，ChatGPT 将在未来发挥更大的作用，为人们带来更加智能的生活体验。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://JrunDing.github.io">JrunDing</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://jrunding.github.io/2023/09/12/ChatGPT%E5%8E%9F%E7%90%86%E7%A7%91%E6%99%AE/">http://jrunding.github.io/2023/09/12/ChatGPT%E5%8E%9F%E7%90%86%E7%A7%91%E6%99%AE/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://JrunDing.github.io" target="_blank">JrunDing</a>！</span></div></div><div class="tag_share"><div class="post_share"><div class="social-share" data-image="https://s1.ax1x.com/2023/03/03/ppkgPxK.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E5%BC%95%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">一	引言</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1ChatGPT%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">1.1	ChatGPT简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E8%83%8C%E6%99%AF"><span class="toc-number">1.2.</span> <span class="toc-text">1.2	语言模型发展背景</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8CChatGPT%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF"><span class="toc-number">2.</span> <span class="toc-text">二	ChatGPT核心技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1GPT-Generative-Pre-trained-Transformer-%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.1.</span> <span class="toc-text">2.1	GPT(Generative Pre-trained Transformer)模型介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2%E9%A2%84%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83-Pre-training-and-Fine-tuning"><span class="toc-number">2.2.</span> <span class="toc-text">2.2	预训练与微调(Pre-training and Fine-tuning)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-1%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1-Pre-training-Tasks"><span class="toc-number">2.2.1.</span> <span class="toc-text">2.3.1	预训练任务(Pre-training Tasks)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-2%E5%BE%AE%E8%B0%83%E4%BB%BB%E5%8A%A1-Fine-tuning-Tasks"><span class="toc-number">2.2.2.</span> <span class="toc-text">2.3.2	微调任务(Fine-tuning Tasks)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3%E4%BB%80%E4%B9%88%E6%98%AF%E5%8F%82%E6%95%B0"><span class="toc-number">2.3.</span> <span class="toc-text">2.3	什么是参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4%E6%9C%80%E5%90%8E%E5%9F%BA%E4%BA%8EGPT%E5%87%BA%E6%9D%A5%E7%9A%84ChatGPT"><span class="toc-number">2.4.</span> <span class="toc-text">2.4	最后基于GPT出来的ChatGPT</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89ChatGPT%E7%9A%84%E4%BC%98%E5%8A%BF%E4%B8%8E%E6%8C%91%E6%88%98"><span class="toc-number">3.</span> <span class="toc-text">三	ChatGPT的优势与挑战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1%E4%BC%98%E5%8A%BF"><span class="toc-number">3.1.</span> <span class="toc-text">3.1	优势</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1%E5%BC%BA%E5%A4%A7%E7%9A%84%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90%E8%83%BD%E5%8A%9B"><span class="toc-number">3.1.1.</span> <span class="toc-text">3.1.1	强大的语言理解与生成能力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2%E9%80%82%E5%BA%94%E5%A4%9A%E9%A2%86%E5%9F%9F%E5%BA%94%E7%94%A8"><span class="toc-number">3.1.2.</span> <span class="toc-text">3.1.2	适应多领域应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2%E6%8C%91%E6%88%98"><span class="toc-number">3.2.</span> <span class="toc-text">3.2	挑战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1%E5%AE%89%E5%85%A8%E6%80%A7%E4%B8%8E%E9%81%93%E5%BE%B7%E9%A3%8E%E9%99%A9"><span class="toc-number">3.2.1.</span> <span class="toc-text">3.2.1	安全性与道德风险</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2%E6%97%B6%E6%95%88%E6%80%A7%E5%92%8C%E4%B8%80%E6%9C%AC%E6%AD%A3%E7%BB%8F%E8%83%A1%E8%AF%B4%E5%85%AB%E9%81%93"><span class="toc-number">3.2.2.</span> <span class="toc-text">3.2.2	时效性和一本正经胡说八道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3%E6%A8%A1%E5%9E%8B%E8%BF%90%E8%A1%8C%E6%95%88%E7%8E%87"><span class="toc-number">3.2.3.</span> <span class="toc-text">3.2.3	模型运行效率</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E6%80%BB%E7%BB%93"><span class="toc-number">4.</span> <span class="toc-text">四	总结</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By JrunDing</div><div class="footer_custom_text">Thanks.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (true){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'http://jrunding.github.io/2023/09/12/ChatGPT%E5%8E%9F%E7%90%86%E7%A7%91%E6%99%AE/'
    this.page.identifier = '/2023/09/12/ChatGPT%E5%8E%9F%E7%90%86%E7%A7%91%E6%99%AE/'
    this.page.title = 'ChatGPT原理科普'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://jrunding.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }

  document.getElementById('darkmode').addEventListener('click', () => {
    setTimeout(() => window.disqusReset(), 200)
  })
}

if ('Disqus' === 'Disqus' || !false) {
  if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script><script>if (window.DISQUSWIDGETS === undefined) {
  var d = document, s = d.createElement('script');
  s.src = 'https://jrunding.disqus.com/count.js';
  s.id = 'dsq-count-scr';
  (d.head || d.body).appendChild(s);
} else {
  DISQUSWIDGETS.getCount({reset: true});
}</script></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>