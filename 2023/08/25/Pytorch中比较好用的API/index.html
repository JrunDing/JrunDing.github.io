<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>Pytorch中比较好用的API | JrunDing</title><meta name="author" content="JrunDing"><meta name="copyright" content="JrunDing"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="本专栏主要介绍Pytorch中比较好用的API  nn.Flatten()​	torch.nn.Flatten(start_dim&#x3D;1, end_dim&#x3D;-1) ​	作用：将连续的维度范围展平为张量 ​	参数：开始维度，结束维度 1234567891011input &#x3D; torch.randn(32, 1, 5, 5)  # 随机数# With default paramet">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch中比较好用的API">
<meta property="og:url" content="http://jrunding.github.io/2023/08/25/Pytorch%E4%B8%AD%E6%AF%94%E8%BE%83%E5%A5%BD%E7%94%A8%E7%9A%84API/index.html">
<meta property="og:site_name" content="JrunDing">
<meta property="og:description" content="本专栏主要介绍Pytorch中比较好用的API  nn.Flatten()​	torch.nn.Flatten(start_dim&#x3D;1, end_dim&#x3D;-1) ​	作用：将连续的维度范围展平为张量 ​	参数：开始维度，结束维度 1234567891011input &#x3D; torch.randn(32, 1, 5, 5)  # 随机数# With default paramet">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s1.ax1x.com/2023/03/03/ppkgkrD.jpg">
<meta property="article:published_time" content="2023-08-25T09:05:40.000Z">
<meta property="article:modified_time" content="2023-10-15T15:41:48.492Z">
<meta property="article:author" content="JrunDing">
<meta property="article:tag" content="Communication, Automation, ML, DL, NLP, CV, Movie, photography">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s1.ax1x.com/2023/03/03/ppkgkrD.jpg"><link rel="shortcut icon" href="https://z1.ax1x.com/2023/11/13/piJMDnH.jpg"><link rel="canonical" href="http://jrunding.github.io/2023/08/25/Pytorch%E4%B8%AD%E6%AF%94%E8%BE%83%E5%A5%BD%E7%94%A8%E7%9A%84API/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: JrunDing","link":"链接: ","source":"来源: JrunDing","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Pytorch中比较好用的API',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-10-15 23:41:48'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://z1.ax1x.com/2023/11/13/piJMDnH.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">240</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/biography/"><span> Biography</span></a></div><div class="menus_item"><a class="site-page" href="/category/"><span> Category</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/movie/"><span> Movie</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="JrunDing"></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/biography/"><span> Biography</span></a></div><div class="menus_item"><a class="site-page" href="/category/"><span> Category</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/movie/"><span> Movie</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Pytorch中比较好用的API</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-25T09:05:40.000Z" title="发表于 2023-08-25 17:05:40">2023-08-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-10-15T15:41:48.492Z" title="更新于 2023-10-15 23:41:48">2023-10-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Deep-Learning/">Deep Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>11分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Pytorch中比较好用的API"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><span class="disqus-comment-count"><a href="http://jrunding.github.io/2023/08/25/Pytorch%E4%B8%AD%E6%AF%94%E8%BE%83%E5%A5%BD%E7%94%A8%E7%9A%84API/#disqus_thread"><i class="fa-solid fa-spinner fa-spin"></i></a></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>本专栏主要介绍Pytorch中比较好用的API</p>
</blockquote>
<h2 id="nn-Flatten"><a href="#nn-Flatten" class="headerlink" title="nn.Flatten()"></a>nn.Flatten()</h2><p>​	<strong>torch.nn.Flatten(start_dim&#x3D;1, end_dim&#x3D;-1)</strong></p>
<p>​	作用：将连续的维度范围展平为张量</p>
<p>​	参数：开始维度，结束维度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">32</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>)  <span class="comment"># 随机数</span></span><br><span class="line"><span class="comment"># With default parameters</span></span><br><span class="line">m = nn.Flatten()  <span class="comment"># 默认维度展开 (1, -1)</span></span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line">output.size()</span><br><span class="line"><span class="comment">#torch.Size([32, 25])</span></span><br><span class="line"><span class="comment"># With non-default parameters</span></span><br><span class="line">m = nn.Flatten(<span class="number">0</span>, <span class="number">2</span>)  <span class="comment"># 指定维度展开 (0, -2)</span></span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line">output.size()</span><br><span class="line"><span class="comment">#torch.Size([160, 5])</span></span><br></pre></td></tr></table></figure>



<h2 id="next-和iter"><a href="#next-和iter" class="headerlink" title="next()和iter()"></a>next()和iter()</h2><p>​	经常会遇到next和iter联合使用，<strong>一般用于取数据时</strong>：<code>X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))</code></p>
<p>​	iter(可迭代对象)</p>
<p>​	补充：可迭代对象Iterable：</p>
<ul>
<li><ul>
<li>一类是：list、tuple、dict、set、str</li>
<li>二类是：generator（都是Iterator对象），包含生成器和带yield的generator function<br>生成器不但可以作用于for，还可以被next函数不断调用并且返回下一个值，可以被next函数不断调用返回下一个值的对象称为迭代器（Iterator）。可迭代的对象如list、dict等需要用iter()函数转化成Iterator</li>
</ul>
</li>
</ul>
<p>​	next(iterator[, default])</p>
<ul>
<li>iterator –可迭代对象</li>
<li>default –可选，用于设置在没有下一个元素时返回该默认值，如果不设置，又没有下一个元素则会触发 StopIteration 异常</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">list_ = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">it = <span class="built_in">iter</span>(list_)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    line = <span class="built_in">next</span>(it)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;第%d 行， %s&quot;</span> %(i, line))</span><br><span class="line"></span><br><span class="line">输出结果：</span><br><span class="line">第<span class="number">0</span> 行， <span class="number">1</span></span><br><span class="line">第<span class="number">1</span> 行， <span class="number">2</span></span><br><span class="line">第<span class="number">2</span> 行， <span class="number">3</span></span><br><span class="line">第<span class="number">3</span> 行， <span class="number">4</span></span><br><span class="line">第<span class="number">4</span> 行， <span class="number">5</span></span><br></pre></td></tr></table></figure>



<h2 id="layer-class-name"><a href="#layer-class-name" class="headerlink" title="layer.__class__.__name__"></a>layer.__class__.__name__</h2><p>​	用于调试神经网络时，可以打印出网络每层的名称和形状</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape: \t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>



<h2 id="summary"><a href="#summary" class="headerlink" title="summary()"></a>summary()</h2><p>​	类似tensorflow中的summary，可以显示使用pytorch写的网络结构</p>
<p>​	<code>pip install torchsummary</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary  <span class="comment"># 导入summary函数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FC</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.liner_1 = nn.Linear(<span class="number">40</span> * <span class="number">40</span>, <span class="number">120</span>)</span><br><span class="line">        self.liner_2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.liner_3 = nn.Linear(<span class="number">84</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        x = <span class="built_in">input</span>.view(-<span class="number">1</span>, <span class="number">40</span> * <span class="number">40</span>)</span><br><span class="line">        x = F.relu(self.liner_1(x))</span><br><span class="line">        x = F.relu(self.liner_2(x))</span><br><span class="line">        x = self.liner_3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = FC().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line">summary(model, (<span class="number">3</span>, <span class="number">40</span>, <span class="number">40</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出</span></span><br><span class="line">FC(</span><br><span class="line">  (liner_1): Linear(in_features=<span class="number">1600</span>, out_features=<span class="number">120</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (liner_2): Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (liner_3): Linear(in_features=<span class="number">84</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">        Layer (<span class="built_in">type</span>)               Output Shape         Param <span class="comment">#</span></span><br><span class="line">================================================================</span><br><span class="line">            Linear-<span class="number">1</span>                  [-<span class="number">1</span>, <span class="number">120</span>]         <span class="number">192</span>,<span class="number">120</span></span><br><span class="line">            Linear-<span class="number">2</span>                   [-<span class="number">1</span>, <span class="number">84</span>]          <span class="number">10</span>,<span class="number">164</span></span><br><span class="line">            Linear-<span class="number">3</span>                    [-<span class="number">1</span>, <span class="number">2</span>]             <span class="number">170</span></span><br><span class="line">================================================================</span><br><span class="line">Total params: <span class="number">202</span>,<span class="number">454</span></span><br><span class="line">Trainable params: <span class="number">202</span>,<span class="number">454</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">Input size (MB): <span class="number">0.02</span></span><br><span class="line">Forward/backward <span class="keyword">pass</span> size (MB): <span class="number">0.00</span></span><br><span class="line">Params size (MB): <span class="number">0.77</span></span><br><span class="line">Estimated Total Size (MB): <span class="number">0.79</span></span><br><span class="line">----------------------------------------------------------------</span><br></pre></td></tr></table></figure>



<h2 id="net-fc和net-fc-weight"><a href="#net-fc和net-fc-weight" class="headerlink" title="net.fc和net.fc.weight"></a>net.fc和net.fc.weight</h2><p>​	net是实例化后的网络，fc是网络中某层的名称</p>
<p>​	<code>net.fc</code>和<code>net.fc.weight</code>分别是直接访问net的某层和该层的权重，可以直接通过赋值来修改层和参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">finetune_net = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)  <span class="comment"># 参数表示不仅定义模型，而且取出在ImageNet上训练好的参数</span></span><br><span class="line">finetune_net.fc = nn.Linear(finetune_net.fc.in_features, <span class="number">2</span>)  <span class="comment"># 修改最后一层</span></span><br><span class="line">nn.init.xavier_uniform_(finetune_net.fc.weight)  <span class="comment"># 只对最后一层的weight做初始化</span></span><br></pre></td></tr></table></figure>



<h2 id="net-children"><a href="#net-children" class="headerlink" title="net.children()"></a>net.children()</h2><p>​	net是实例化后的网络，children()方法是提取出该对象的所有层，通常的用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span>(net.children())[-<span class="number">3</span>:]  <span class="comment"># 取出网络的最后三层</span></span><br></pre></td></tr></table></figure>



<h2 id="net-add-module"><a href="#net-add-module" class="headerlink" title="net.add_module()"></a>net.add_module()</h2><p>​	网络添加层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.add_module(<span class="string">&#x27;transpose_conv&#x27;</span>, nn.ConvTranspose2d(num_classes, num_classes, kernel_size=<span class="number">64</span>, padding=<span class="number">16</span>, stride=<span class="number">32</span>))</span><br></pre></td></tr></table></figure>



<h2 id="tensor-detach"><a href="#tensor-detach" class="headerlink" title="tensor.detach()"></a>tensor.detach()</h2><p>​	通常用在冻结网络部分结构</p>
<p>​	返回一个新的<code>tensor</code>，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个<code>tensor</code>永远不需要计算其梯度，不具有grad。<strong>即使之后重新将它的requires_grad置为true,它也不会具有梯度grad</strong></p>
<p>​	这样我们就会继续使用这个新的tensor进行计算，后面当我们进行反向传播时，到该调用detach()的<code>tensor</code>就会停止，不能再继续向前进行传播</p>
<p>​	注：使用detach返回的<code>tensor</code>和原始的<code>tensor</code>共同一个<code>内存，即一个修改另一个也会跟着改变</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"> </span><br><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br><span class="line">out = a.sigmoid()</span><br><span class="line"> </span><br><span class="line">out.<span class="built_in">sum</span>().backward()</span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;返回：</span></span><br><span class="line"><span class="string">None</span></span><br><span class="line"><span class="string">tensor([0.1966, 0.1050, 0.0452])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"> </span><br><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br><span class="line">out = a.sigmoid()</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#添加detach(),c的requires_grad为False</span></span><br><span class="line">c = out.detach()</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#这时候没有对c进行更改，所以并不会影响backward()</span></span><br><span class="line">out.<span class="built_in">sum</span>().backward()</span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br><span class="line"> </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;返回：</span></span><br><span class="line"><span class="string">None</span></span><br><span class="line"><span class="string">tensor([0.7311, 0.8808, 0.9526], grad_fn=&lt;SigmoidBackward&gt;)</span></span><br><span class="line"><span class="string">tensor([0.7311, 0.8808, 0.9526])</span></span><br><span class="line"><span class="string">tensor([0.1966, 0.1050, 0.0452])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>​	从上可见tensor c是由out分离得到的，但是我也没有去改变这个c，这个时候依然对原来的out求导是不会有错误的，即c,out之间的区别是c是没有梯度的，out是有梯度的,但是需要注意的是下面两种情况是汇报错的</p>
<p>​	① <strong>当使用detach()分离tensor，然后用这个分离出来的tensor去求导数，会影响backward()，会出现错误</strong></p>
<p>​	② <strong>当使用detach()分离tensor并且更改这个tensor时，即使再对原来的out求导数，会影响backward()，会出现错误</strong></p>
<h2 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat()"></a>torch.cat()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">0</span>)  <span class="comment"># 第0维，最外层</span></span><br><span class="line">tensor([[ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">1</span>)  <span class="comment"># 第1维，最内层</span></span><br><span class="line">tensor([[ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>,  <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>,  <span class="number">0.6580</span>,</span><br><span class="line">         -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>, -<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>, -<span class="number">0.1034</span>,</span><br><span class="line">         -<span class="number">0.5790</span>,  <span class="number">0.1497</span>]])</span><br></pre></td></tr></table></figure>



<h2 id="torch-squeeze-和torch-unsqueeze"><a href="#torch-squeeze-和torch-unsqueeze" class="headerlink" title="torch.squeeze()和torch.unsqueeze()"></a>torch.squeeze()和torch.unsqueeze()</h2><p>​	<code>torch.squeeze(A, N)</code></p>
<p>​	减少张量A指定位置N的维度，如果张量A维度为(1, 1, 3)，执行<code>torch.squeeze(A, 1)</code>后A的维度变为(1, 3)</p>
<p>​	如果指定的维度大于1，那么操作无效</p>
<p>​	如果不指定维度N，那么将删除所有维度为1的维度</p>
<p>​	<code>torch.unsqueeze(A, N)</code></p>
<p>​	增加数组A指定位置N的维度，例如两行三列的数组A维度为(2，3)，那么这个数组就有三个位置可以增加维度，分别是（ [位置0] 2，[位置1] 3 [位置2] ）或者是 （ [位置-3] 2，[位置-2] 3 [位置-1] ），如果执行 torch.unsqueeze(A，1)，数据的维度就变为了 （2，1，3）</p>
<h2 id="to-device"><a href="#to-device" class="headerlink" title=".to(device)"></a>.to(device)</h2><p>​	torch.tensor的本质是对象，所有的tensor对象都具有这个方法，可将tensor数据放入gpu上运行</p>
<p>​	torch中定义的变量、常量默认都在cpu上，如果想放在GPU上，就需要<code>data.to(device)</code>来指明</p>
<p>​	将GPU上的数据搬回CPU，可以使用<code>.to(device)</code>或者直接<code>data.cpu()</code>，第二种方式需要注意data必须是no_grad的状态，可以使用<code>with torch.no_grad():</code>来指明</p>
<p>​	device的常用写法：<code>device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</code></p>
<h2 id="Module类"><a href="#Module类" class="headerlink" title="Module类"></a>Module类</h2><p>​	注意，在Pytorch中，to the best of my knowledge，<code>torch.nn</code>提供的所有模块、网络都是继承<code>nn.Module</code>，这意味着在Pytorch中没有像Tensorflow中的layer概念，直接是全部的Module拼接在一起，那么我们可以选择性地保存部分Module参数</p>
<p>​	在Pytorch中，只有继承自Module类的子类，程序才会自动地、默认地处理batch这个维度，我们只需要专心对除了batch以外的维度进行操作即可，因为Module类帮我们处理了batch</p>
<h2 id="复数处理"><a href="#复数处理" class="headerlink" title="复数处理"></a>复数处理</h2><p>​	在Pytorch中，如果需要在网络中使用到复数的相关计算，会提示无法实现复数的微分，因此涉及到复数处理部分，可以使用<code>with torch.no_grad():</code>让这部分 不用计算梯度，处理完成后，再为某tensor数据使用<code>data.requires_grad_(True)</code>方法，使下面的操作再次需要计算梯度。这意味着只要父类数据需要计算梯度或者父类数据处于GPU上，那么由它计算得出的子类数据也都是需要计算梯度并且处于GPU上的</p>
<p>​	<strong>但是请注意！！！上述处理方法尽可能少使用，因为如果网络需要grad的变量之间相隔较远，会极大影响网络性能，甚至导致不收敛！</strong></p>
<h2 id="torch-nn-ConvTranspose2d"><a href="#torch-nn-ConvTranspose2d" class="headerlink" title="torch.nn.ConvTranspose2d"></a>torch.nn.ConvTranspose2d</h2><p>​	使用前提：stride &gt; 1</p>
<p>​	<strong>是通过padding使得卷积之后输出的特征图大小保持不变(相对于输入特征图)，不代表得到的输出特征图的大小与输入特征图的大小完全相同，而是他们之间的比例保持为 输入特征图大小&#x2F;输出特征图大小 &#x3D; stride</strong></p>
<p>​	比如输入特征图为6<em>6，stride&#x3D;2, kernel_size &#x3D; 3, 所以进行same卷机操作得输出特征图为3</em>3 (6&#x2F;2 &#x3D; 3)</p>
<p>​	如果输入特征图为5<em>5，stride&#x3D;2，kernel_size &#x3D; 3,这时候设置padding &#x3D; 1，那么也会得到输出特征图为3</em>3</p>
<p>​	那么这样的情况就会导致在逆卷积时出现一个问题。</p>
<p>​	问题就是，不同大小的图片经过卷积运算能得到相同尺寸的输出，那么作为逆运算，<strong>同样的一张输入图像经过反卷积是否会有不同尺寸的合法输出</strong>？这样的话就存在争议了</p>
<p>​	上面还只是进行same卷积的情况，如果考虑valid卷积，stride&#x3D;2, kernel_size &#x3D; 3，padding&#x3D;0时，输入特征图为7<em>7和8</em>8的结果也是3*3</p>
<p>​	解决争议的办法就是使用output_padding参数</p>
<p>​	output_padding的作用是：</p>
<p>​	当stride &gt; 1时，Conv2d将多个输入形状映射到相同的输出形状。output_padding通过在一边有效地增加计算出的输出形状来解决这种模糊性。</p>
<p>​	首先我们要认同一个前提：</p>
<p>​	**大多数情况下我们都希望经过卷积&#x2F;反卷积处理后的图像尺寸比例与步长相等，即**输入特征图大小&#x2F;输出特征图大小 &#x3D; stride**，也就是same模式**。</p>
<p>​	所以我们只要通过添加output_padding这一参数来使得结果满足这一前提，那么输出的图片的大小就能够保证为输入图片*stride的大小,而不是任意可能的大小</p>
<p>​	实现办法：</p>
<p>​	因为pytorch将参数padding（注意与output_padding区别）建议设置为(kernel_size - 1)&#x2F;2，由式子padding&#x3D; kernel - 1 - padding转换而来</p>
<p>​	当我们希望得到****输入特征图大小&#x2F;输出特征图大小 &#x3D; stride****的话，代入上面的式子能够得到结果：</p>
<p>​	padding &#x3D; (kernel_size - stride + output_padding )&#x2F;2</p>
<p>​	所以为了让padding &#x3D; (kernel_size - 1)&#x2F;2，则output_padding应该取值为stride - 1，这样就能够满足<em><strong>*输入特征图大小&#x2F;输出特征图大小 &#x3D; stride*</strong></em></p>
<p>​	当然，你可以取别的值，这并不会影响到逆卷积的计算，但是在后面进行有关大小的操作时就很可能出现问题，因为输出的图片的大小并不能保证是 输入图片<em>stride的大小，可能是任意正确的大小，如上面举的例子，可能是7</em>7或8*8等</p>
<h2 id="torch-einsum"><a href="#torch-einsum" class="headerlink" title="torch.einsum()"></a>torch.einsum()</h2><p>​	ein 就是爱因斯坦的ein，sum就是求和。einsum就是爱因斯坦求和约定，其实作用就是把求和符号省略</p>
<p>​	<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=pkVwUVEHmfI">https://www.youtube.com/watch?v=pkVwUVEHmfI</a></p>
<p>​	参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/zhaohongfei_358/article/details/125273126?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1-125273126-blog-96462827.235%5Ev38%5Epc_relevant_anti_vip_base&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1-125273126-blog-96462827.235%5Ev38%5Epc_relevant_anti_vip_base&utm_relevant_index=1">https://blog.csdn.net/zhaohongfei_358/article/details/125273126?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-125273126-blog-96462827.235%5Ev38%5Epc_relevant_anti_vip_base&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-125273126-blog-96462827.235%5Ev38%5Epc_relevant_anti_vip_base&amp;utm_relevant_index=1</a></p>
<h2 id="torch-permute"><a href="#torch-permute" class="headerlink" title="torch.permute()"></a>torch.permute()</h2><p>​	用于交换tensor维度，和transpose很像，但是它一步到位</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">a = a.permute(<span class="number">1</span>, <span class="number">0</span>) <span class="comment"># 将a的0维和1维交换</span></span><br><span class="line">b.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>) <span class="comment"># 可以任意交换维度，数字就是在原tensor中的维度索引</span></span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://JrunDing.github.io">JrunDing</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://jrunding.github.io/2023/08/25/Pytorch%E4%B8%AD%E6%AF%94%E8%BE%83%E5%A5%BD%E7%94%A8%E7%9A%84API/">http://jrunding.github.io/2023/08/25/Pytorch%E4%B8%AD%E6%AF%94%E8%BE%83%E5%A5%BD%E7%94%A8%E7%9A%84API/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://JrunDing.github.io" target="_blank">JrunDing</a>！</span></div></div><div class="tag_share"><div class="post_share"><div class="social-share" data-image="https://s1.ax1x.com/2023/03/03/ppkgkrD.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#nn-Flatten"><span class="toc-number">1.</span> <span class="toc-text">nn.Flatten()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#next-%E5%92%8Citer"><span class="toc-number">2.</span> <span class="toc-text">next()和iter()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#layer-class-name"><span class="toc-number">3.</span> <span class="toc-text">layer.__class__.__name__</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#summary"><span class="toc-number">4.</span> <span class="toc-text">summary()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#net-fc%E5%92%8Cnet-fc-weight"><span class="toc-number">5.</span> <span class="toc-text">net.fc和net.fc.weight</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#net-children"><span class="toc-number">6.</span> <span class="toc-text">net.children()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#net-add-module"><span class="toc-number">7.</span> <span class="toc-text">net.add_module()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tensor-detach"><span class="toc-number">8.</span> <span class="toc-text">tensor.detach()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-cat"><span class="toc-number">9.</span> <span class="toc-text">torch.cat()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-squeeze-%E5%92%8Ctorch-unsqueeze"><span class="toc-number">10.</span> <span class="toc-text">torch.squeeze()和torch.unsqueeze()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#to-device"><span class="toc-number">11.</span> <span class="toc-text">.to(device)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Module%E7%B1%BB"><span class="toc-number">12.</span> <span class="toc-text">Module类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%8D%E6%95%B0%E5%A4%84%E7%90%86"><span class="toc-number">13.</span> <span class="toc-text">复数处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-nn-ConvTranspose2d"><span class="toc-number">14.</span> <span class="toc-text">torch.nn.ConvTranspose2d</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-einsum"><span class="toc-number">15.</span> <span class="toc-text">torch.einsum()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-permute"><span class="toc-number">16.</span> <span class="toc-text">torch.permute()</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By JrunDing</div><div class="footer_custom_text">Thanks.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (true){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'http://jrunding.github.io/2023/08/25/Pytorch%E4%B8%AD%E6%AF%94%E8%BE%83%E5%A5%BD%E7%94%A8%E7%9A%84API/'
    this.page.identifier = '/2023/08/25/Pytorch%E4%B8%AD%E6%AF%94%E8%BE%83%E5%A5%BD%E7%94%A8%E7%9A%84API/'
    this.page.title = 'Pytorch中比较好用的API'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://jrunding.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }

  document.getElementById('darkmode').addEventListener('click', () => {
    setTimeout(() => window.disqusReset(), 200)
  })
}

if ('Disqus' === 'Disqus' || !false) {
  if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script><script>if (window.DISQUSWIDGETS === undefined) {
  var d = document, s = d.createElement('script');
  s.src = 'https://jrunding.disqus.com/count.js';
  s.id = 'dsq-count-scr';
  (d.head || d.body).appendChild(s);
} else {
  DISQUSWIDGETS.getCount({reset: true});
}</script></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>